{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Snm0JaShoXWn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertModel, BertTokenizer, get_linear_schedule_with_warmup\n",
        "from torch.optim import AdamW\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# 多分类标签编码处理函数（不进行 fake/real 映射）\n",
        "def load_and_prepare_multiclass_data(train_path, val_path, test_path):\n",
        "    train_df = pd.read_csv(train_path)\n",
        "    val_df = pd.read_csv(val_path)\n",
        "    test_df = pd.read_csv(test_path)\n",
        "\n",
        "    label_encoder = LabelEncoder()\n",
        "    label_encoder.fit(train_df[\"label\"])  # 使用训练集fit编码器\n",
        "\n",
        "    for df in [train_df, val_df, test_df]:\n",
        "        df[\"label\"] = label_encoder.transform(df[\"label\"])\n",
        "\n",
        "    return train_df, val_df, test_df, label_encoder.classes_\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "\n",
        "def predict(model, dataset, batch_size=16):\n",
        "    loader = DataLoader(dataset, batch_size=batch_size)\n",
        "    model.eval()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
        "            outputs = model(**inputs)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            predictions.extend(preds.cpu().numpy())\n",
        "\n",
        "    return predictions\n",
        "\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "def analyze_predictions(preds, test_df, label_col=\"label\", label_map=None, top_k=5, output_file=\"misclassified_samples.csv\"):\n",
        "    if label_map is None:\n",
        "        # 默认label_map为 index 到字符串的映射\n",
        "        label_map = {i: str(i) for i in sorted(set(test_df[label_col].tolist()))}\n",
        "\n",
        "    true_labels = test_df[label_col].tolist()\n",
        "\n",
        "    print(\"📊 Classification Report:\")\n",
        "    print(classification_report(true_labels, preds, target_names=[label_map[i] for i in sorted(label_map)]))\n",
        "\n",
        "    # 混淆矩阵\n",
        "    cm = confusion_matrix(true_labels, preds)\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=[label_map[i] for i in sorted(label_map)],\n",
        "                yticklabels=[label_map[i] for i in sorted(label_map)])\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.show()\n",
        "\n",
        "    # 错误样本\n",
        "    test_df[\"true_label\"] = test_df[label_col]\n",
        "    test_df[\"predicted_label\"] = preds\n",
        "    test_df[\"correct\"] = test_df[\"true_label\"] == test_df[\"predicted_label\"]\n",
        "\n",
        "    mistakes = test_df[~test_df[\"correct\"]].copy()\n",
        "    mistakes[\"true_label_name\"] = mistakes[\"true_label\"].map(label_map)\n",
        "    mistakes[\"predicted_label_name\"] = mistakes[\"predicted_label\"].map(label_map)\n",
        "\n",
        "    print(f\"\\n❌ Top {top_k} Misclassified Samples:\\n\")\n",
        "    for i, row in mistakes.head(top_k).iterrows():\n",
        "        print(f\"[{i}] Statement: {row['clean_statement_bert']}\")\n",
        "        print(f\"    ➤ True: {row['true_label_name']} | Pred: {row['predicted_label_name']}\")\n",
        "        print(\"\")\n",
        "\n",
        "    # 保存到文件\n",
        "    mistakes[[\"clean_statement_bert\", \"true_label_name\", \"predicted_label_name\"]].to_csv(output_file, index=False)\n",
        "    print(f\"\\n📝 Misclassified samples saved to: {output_file}\")\n",
        "\n",
        "    return test_df\n",
        "\n",
        "# ---------- Dataset ----------\n",
        "class MultiClassFakeNewsDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.texts = dataframe[\"clean_statement_bert\"].tolist()\n",
        "        self.labels = dataframe[\"label\"].tolist()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        encodings = self.tokenizer(\n",
        "            self.texts[idx],\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encodings['input_ids'].squeeze(0),\n",
        "            'attention_mask': encodings['attention_mask'].squeeze(0),\n",
        "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }# ---------- Multi-Class BERT Model ----------\n",
        "class MultiClassBERTClassifier(nn.Module):\n",
        "    def __init__(self, model_name, num_labels=6, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained(model_name)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        cls_output = outputs.pooler_output\n",
        "        return self.classifier(self.dropout(cls_output))\n",
        "# ---------- Training Function for Multi-Class ----------\n",
        "def train_multiclass_model(train_dataset, val_dataset, model, epochs=6, batch_size=16, lr=2e-5, warmup_ratio=0.1, patience=2):\n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    total_steps = len(train_loader) * epochs\n",
        "    warmup_steps = int(warmup_ratio * total_steps)\n",
        "\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
        "\n",
        "    train_losses, val_accuracies = [], []\n",
        "    best_val_acc = 0\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(**inputs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            loss.backward()\n",
        "            clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        train_losses.append(avg_loss)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        correct, total = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n",
        "                labels = batch['labels'].to(device)\n",
        "                outputs = model(**inputs)\n",
        "                preds = torch.argmax(outputs, dim=1)\n",
        "                correct += (preds == labels).sum().item()\n",
        "                total += len(labels)\n",
        "        acc = correct / total\n",
        "        val_accuracies.append(acc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Train loss={avg_loss:.4f}, Val acc={acc:.4f}\")\n",
        "\n",
        "        if acc > best_val_acc:\n",
        "            best_val_acc = acc\n",
        "            patience_counter = 0\n",
        "            torch.save(model.state_dict(), \"best_multiclass_model.pt\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(\"Early stopping.\")\n",
        "                break\n",
        "\n",
        "    plt.plot(range(1, len(train_losses)+1), train_losses, label=\"Train Loss\")\n",
        "    plt.plot(range(1, len(val_accuracies)+1), val_accuracies, label=\"Validation Acc\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Training Curve\")\n",
        "    plt.show()\n",
        "\n",
        "    return model\n",
        "\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# 加载数据\n",
        "train_df, val_df, test_df, class_names = load_and_prepare_multiclass_data(\n",
        "    \"/content/processed_train (2).csv\",\n",
        "    \"/content/processed_valid (2).csv\",\n",
        "    \"/content/processed_test (2).csv\"\n",
        ")\n",
        "\n",
        "# Tokenizer & Dataset\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "train_set = MultiClassFakeNewsDataset(train_df, tokenizer, max_len=128)\n",
        "val_set = MultiClassFakeNewsDataset(val_df, tokenizer, max_len=128)\n",
        "\n",
        "# 模型\n",
        "model = MultiClassBERTClassifier(\"bert-base-uncased\", num_labels=6)\n",
        "\n",
        "# 训练\n",
        "model = train_multiclass_model(train_set, val_set, model, epochs=10, batch_size=16)\n",
        "\n",
        "# 构建测试集数据集\n",
        "test_set = MultiClassFakeNewsDataset(test_df, tokenizer, max_len=128)\n",
        "\n",
        "\n",
        "# 加载最优模型参数（可选）\n",
        "model.load_state_dict(torch.load(\"best_multiclass_model.pt\"))\n",
        "\n",
        "# 预测\n",
        "preds = predict(model, test_set)\n",
        "\n",
        "# 评估与可视化\n",
        "analyze_predictions(preds, test_df)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}